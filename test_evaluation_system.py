#!/usr/bin/env python3
"""
Test the evaluation system functionality
"""

import os
import sys
from datetime import datetime

# Add the app directory to sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def test_evaluation_system():
    """Test the evaluation system functionality"""
    
    try:
        # Import Flask app components
        from app import app, db, PublishedModel, PublishedModelEvaluation, get_model_quality_scores, _get_evaluation_data
        
        with app.app_context():
            # Get a sample model
            sample_model = PublishedModel.query.first()
            
            if not sample_model:
                print("‚ùå No models found in database")
                return False
            
            print(f"üîç Testing model: {sample_model.name}")
            
            # Test get_model_quality_scores function
            print("\\nüìä Testing get_model_quality_scores...")
            scores = get_model_quality_scores(sample_model.name)
            
            print(f"‚úÖ Retrieved {len(scores)} scoring categories:")
            for score in scores:
                print(f"   ‚Ä¢ {score['name']}: {score['score']}/5 - {score['explanation'][:60]}...")
            
            # Test evaluation data retrieval
            print("\\nüìà Testing _get_evaluation_data...")
            evaluation_data = _get_evaluation_data(sample_model)
            
            if evaluation_data:
                print("‚úÖ Evaluation data retrieved successfully:")
                print(f"   ‚Ä¢ Risk & Return: {evaluation_data.get('risk_return', 'N/A')}/5")
                print(f"   ‚Ä¢ Data Quality: {evaluation_data.get('data_quality', 'N/A')}/5")
                print(f"   ‚Ä¢ Model Logic: {evaluation_data.get('model_logic', 'N/A')}/5")
                print(f"   ‚Ä¢ Code Quality: {evaluation_data.get('code_quality', 'N/A')}/5")
                print(f"   ‚Ä¢ Testing & Validation: {evaluation_data.get('testing_validation', 'N/A')}/5")
                print(f"   ‚Ä¢ Governance & Compliance: {evaluation_data.get('governance_compliance', 'N/A')}/5")
                print(f"   ‚Ä¢ Composite Score: {evaluation_data.get('composite_score', 'N/A')}/100")
                print(f"   ‚Ä¢ Method: {evaluation_data.get('method', 'N/A')}")
            else:
                print("‚ùå No evaluation data retrieved")
                return False
            
            # Test database evaluation record
            print("\\nüíæ Testing database evaluation record...")
            db_evaluation = PublishedModelEvaluation.query.filter_by(published_model_id=sample_model.id).first()
            
            if db_evaluation:
                print("‚úÖ Database evaluation record found:")
                print(f"   ‚Ä¢ Created: {db_evaluation.created_at}")
                print(f"   ‚Ä¢ Composite Score: {db_evaluation.composite_score}/100")
                print(f"   ‚Ä¢ Method: {db_evaluation.method}")
                print(f"   ‚Ä¢ Rationale Preview: {db_evaluation.rationale_preview}")
            else:
                print("‚ùå No database evaluation record found")
                return False
            
            # Test multiple models
            print("\\nüîÑ Testing multiple models...")
            models = PublishedModel.query.limit(5).all()
            evaluation_count = 0
            
            for model in models:
                eval_data = _get_evaluation_data(model)
                if eval_data and eval_data.get('composite_score'):
                    evaluation_count += 1
                    print(f"   ‚úÖ {model.name}: {eval_data['composite_score']}/100")
            
            print(f"\\nüìä Summary: {evaluation_count}/{len(models)} models have valid evaluations")
            
            # Test total evaluation count
            total_evaluations = PublishedModelEvaluation.query.count()
            total_models = PublishedModel.query.count()
            
            print(f"üìà Database Summary:")
            print(f"   ‚Ä¢ Total Models: {total_models}")
            print(f"   ‚Ä¢ Total Evaluations: {total_evaluations}")
            print(f"   ‚Ä¢ Coverage: {(total_evaluations/total_models)*100:.1f}%")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Error testing evaluation system: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    print("ML Model Evaluation System Test")
    print("=" * 40)
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    success = test_evaluation_system()
    
    print()
    if success:
        print("üéâ All evaluation system tests passed!")
        print("‚úÖ The 6-category scoring system is fully operational")
        print("üåê Models can be viewed at: http://127.0.0.1:5009/published")
    else:
        print("‚ùå Some tests failed - please check the implementation")
